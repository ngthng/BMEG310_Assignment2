
---
title: "assignment1"
author: "Theo-60985751"
date: "2023-10-12"
output: html_document
---

## Loading the data

```{r}
ovarian.dataset <- read.delim("ovarian.data", sep=",", header = FALSE)
features <- c("perimeter", "area", "smoothness", "symmetry", "concavity", paste("protein", seq(1, 25), sep=""))

names(ovarian.dataset) <- c("cell_id", "diagnosis", features) # paste0(features,"_mean"), paste0(features,"_se"), paste0(features,"_worst")

head(ovarian.dataset)
```


## Question 1

# 1.1
```{r}
data_subset <- ovarian.dataset[, c(3:ncol(ovarian.dataset))]

pca_result <- prcomp(data_subset, center = TRUE, scale. = TRUE)

pca_summary <- summary(pca_result)

proportion_variance_PC1 <- pca_summary$importance[2, 1]

cat("Number of variation in the data associated with PC1: ",proportion_variance_PC1, "\n" )

```

# 1.2
```{r}
cumulative_variances <- pca_summary$importance[3, ]

num_pcs_for_90_percent_variance <- which(cumulative_variances >= 0.90)[1]

cat("Number of PCs needed to preserve 90% of the variance: ", num_pcs_for_90_percent_variance, "\n")

```

# 1.3
```{r}
library(ggplot2)

pca_data <- as.data.frame(pca_result$x[, c(1, 2)])

ggplot(pca_data, aes(x = PC1, y = PC2)) +
  geom_point(aes(color = ovarian.dataset$diagnosis)) +
  labs(title = "PCA: PC1 vs. PC2", x = "PC1", y = "PC2", color = "Diagnosis")  
```

# 1.4
```{r}
ggplot(ovarian.dataset, aes(x = area, y = concavity, color = diagnosis)) +
  geom_point() +
  labs(title = "Scatter Plot of Area vs. Concavity", x = "Area", y = "Concavity")
```

# 1.5

The PCA plot provides a visualization of how well the first two PCs separate the classes in a lower-dimensional space. The "Area" vs. "Concavity" plot shows the direct relationship between these two specific features. It does not perform dimensionality reduction or create a new feature space. The PCA plot gives a better separation between the classes because it shows less overlap between the class data points, meaning that the data points from one class are more clustered together, and there is less mixing of data points from different classes. This clearer separation makes it easier to visually distinguish between classes.  

# 1.6 bonus

```{r}
pca_data <- pca_result$x

boxplot(pca_data, main = "Distribution of PCs", xlab = "PC", ylab = "Score")
```

## Question 2

# 2.1
```{r}
scaled_data <- scale(data_subset)
kmeans_result <- kmeans(scaled_data, centers = 2)

kmeans_clusters <- kmeans_result$cluster

predicted_diagnosis <- ifelse(cluster_assignments == 1, "B", "M")

true_labels <- ovarian.dataset$diagnosis

confusion_matrix <- table(Predicted = predicted_diagnosis, True = true_labels)


```


## Question 3

Dividing the data into training and testing data

```{r}
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]
  
```


Converting the labels into 1 and 0

```{r}
train_data = ovarian.dataset.train

for (i in 1:nrow(train_data)){
  train_data$diagnosis[i] = ifelse(train_data$diagnosis[i] == "M", 1, 0)
}
train_data[,2] = as.integer(train_data[,2])
train_datasubset = train_data[,c(2:ncol(train_data))]

test_data = ovarian.dataset.test
for (i in 1:nrow(test_data)){
  test_data$diagnosis[i] = ifelse(test_data$diagnosis[i] == "M", 1, 0)
}
test_data[,2] = as.integer(test_data[,2])
test_datasubset = test_data[, c(2:ncol(test_data))]

```

# 3.1
Building logistical classifier:

```{r}

glm.fit = glm(diagnosis ~ perimeter + area + smoothness + symmetry + concavity + protein1 + protein10 + protein11 + protein12 + protein13 + protein14 + protein15 + protein16 + protein17 + protein18 + protein19 + protein2 + protein20 + protein21 + protein22 + protein23 + protein24 + protein25 + protein3 + protein4 + protein5 + protein6 + protein7 + protein8 + protein9 , data = train_datasubset, family = binomial )
```

Fitting with the test data

```{r}
glm.probs = predict(glm.fit, test_datasubset, type = "response")
glm.probs[1:5]

glm.pred = ifelse(glm.probs > 0.5, 1, 0)

attach(test_datasubset)
table(glm.pred, diagnosis)

mean(glm.pred == diagnosis)

```
Classification rate (accuracy) = 93.61%
Precision: 120/(120 + 11) = 91.6%
Recall: 120/(120 + 9) = 93.02%

# Q3.2

Applying PCA on the train data set

``` {r}
pca_train_result = prcomp(train_datasubset, center = TRUE, scale. = TRUE)

summary(pca_train_result)

top5PC_data_train = as.data.frame(pca_train_result$x[, c(1:5)])
```

Building logistical classifier and fit with the top 5 PCs:
```{r}
glmTop5.fit = glm(PC1 ~ PC2 + PC3 + PC4 + PC5, data = top5PC_data_train,family = binomial)

glmTop5.probs = predict(glmTop5.fit, test_datasubset, type = "response")

```


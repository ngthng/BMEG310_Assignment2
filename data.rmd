
---
title: "assignment2"
author: "Theo-60985751"
date: "2023-10-12"
output: html_document
---

## Loading the data

```{r}
ovarian.dataset <- read.delim("ovarian.data", sep=",", header = FALSE)
features <- c("perimeter", "area", "smoothness", "symmetry", "concavity", paste("protein", seq(1, 25), sep=""))

names(ovarian.dataset) <- c("cell_id", "diagnosis", features) # paste0(features,"_mean"), paste0(features,"_se"), paste0(features,"_worst")

head(ovarian.dataset)
```


## Question 1

# 1.1
```{r}
data_subset <- ovarian.dataset[, c(3:ncol(ovarian.dataset))]

pca_result <- prcomp(data_subset, center = TRUE, scale. = TRUE)

pca_summary <- summary(pca_result)

proportion_variance_PC1 <- pca_summary$importance[2, 1]

cat("Number of variation in the data associated with PC1: ",proportion_variance_PC1, "\n" )

```

# 1.2
```{r}
cumulative_variances <- pca_summary$importance[3, ]

num_pcs_for_90_percent_variance <- which(cumulative_variances >= 0.90)[1]

cat("Number of PCs needed to preserve 90% of the variance: ", num_pcs_for_90_percent_variance, "\n")

```

# 1.3
```{r}
library(ggplot2)

pca_data <- as.data.frame(pca_result$x[, c(1, 2)])

ggplot(pca_data, aes(x = PC1, y = PC2)) +
  geom_point(aes(color = ovarian.dataset$diagnosis)) +
  labs(title = "PCA: PC1 vs. PC2", x = "PC1", y = "PC2", color = "Diagnosis")  
```

# 1.4
```{r}
ggplot(ovarian.dataset, aes(x = area, y = concavity, color = diagnosis)) +
  geom_point() +
  labs(title = "Scatter Plot of Area vs. Concavity", x = "Area", y = "Concavity")
```

# 1.5

The PCA plot provides a visualization of how well the first two PCs separate the classes in a lower-dimensional space. The "Area" vs. "Concavity" plot shows the direct relationship between these two specific features. It does not perform dimensionality reduction or create a new feature space. The PCA plot gives a better separation between the classes because it shows less overlap between the class data points, meaning that the data points from one class are more clustered together, and there is less mixing of data points from different classes. This clearer separation makes it easier to visually distinguish between classes.  

# 1.6 bonus

```{r}
pca_data <- pca_result$x

boxplot(pca_data, main = "Distribution of PCs", xlab = "PC", ylab = "Score")
```

## Question 2

# 2.1
```{r}
scaled_data <- scale(data_subset)
kmeans_result <- kmeans(scaled_data, centers = 2)

kmeans_clusters <- kmeans_result$cluster

predicted_labels <- ifelse(kmeans_clusters == 1, "B", "M")

true_labels <- ovarian.dataset$diagnosis

confusion_matrix <- table(Predicted = predicted_labels, True = true_labels)

confusion_matrix

TP <- sum(predicted_labels == "M" & true_labels == "M")
TN <- sum(predicted_labels == "B" & true_labels == "B")
FP <- sum(predicted_labels == "M" & true_labels == "B")
FN <- sum(predicted_labels == "B" & true_labels == "M")

accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

cat("Accuracy: ", accuracy, "\n")
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
```
# 2.2
```{r}
num_runs <- 10

accuracy_results <- numeric(num_runs)

for (i in 1:num_runs) {
  
  kmeans_result <- kmeans(scaled_data, centers = 2)
  
  kmeans_clusters <- kmeans_result$cluster
  
  predicted_labels <- ifelse(kmeans_clusters == 1, "B", "M")
  
  TP <- sum(predicted_labels == "M" & true_labels == "M")
  TN <- sum(predicted_labels == "B" & true_labels == "B")
  FP <- sum(predicted_labels == "M" & true_labels == "B")
  FN <- sum(predicted_labels == "B" & true_labels == "M")
  
  accuracy <- (TP + TN) / (TP + TN + FP + FN)
  
  accuracy_results[i] <- accuracy
  cat("Accuracy Across Run", i, ": ", accuracy_results[i], "\n")
}

mean_accuracy <- mean(accuracy_results)

cat("Mean Accuracy Across 10 Runs: ", mean_accuracy, "\n")

```
The mean accuracy is different from one run to another due to the sensitivity of kmeans clustering to the initial placement of cluster centroids. In kmeans clustering, cluster centroids are initialized randomly which means that the initial positions of centroids may vary in different runs of the algorithm. These initial positions have a significant impact on how data points are assigned to clusters. Therefore, the assignment of data points to the "B" or "M" cluster can vary between runs (see Q2.1: predicted "B" and "M" are changing/alternating each run)

# 2.3
```{r}
num_runs <- 10

true_labels <- ovarian.dataset$diagnosis

top_5_pcs <- pca_result$x[, 1:5]


for (i in 1:num_runs){
  
  kmeans_result <- kmeans(top_5_pcs, centers = 2)

  cluster_assignments <- kmeans_result$cluster

  predicted_labels <- ifelse(cluster_assignments == 1, "B", "M")
  
  TP <- sum(predicted_labels == "M" & true_labels == "M")
  TN <- sum(predicted_labels == "B" & true_labels == "B")
  FP <- sum(predicted_labels == "M" & true_labels == "B")
  FN <- sum(predicted_labels == "B" & true_labels == "M")
  
  accuracy <- (TP + TN) / (TP + TN + FP + FN)
  
  accuracy_results[i] <- accuracy
   cat("Accuracy Across Run", i, ": ", accuracy_results[i], "\n")
}

mean_accuracy <- mean(accuracy_results)

cat("Accuracy Across 10 Runs with the Top 5 PCs: ", mean_accuracy, "\n")

```
# 2.4
The mean accuracy is very similar between the kmeans analysis on the original data and the kmeans analysis on the top 5 PCs because of the effective PCA dimensionality reduction, which retains essential clustering information. This shows that dimensionality reduction via PCA did not significantly hinder the clustering task, resulting in comparable clustering performance between two approaches. Multiple initialization runs (running kmeans clustering 10 times) also contribute to this similarity. 

## Question 3

Dividing the data into training and testing data

```{r}
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]
  
```


Converting the labels into 1 and 0

```{r}
train_data <- ovarian.dataset.train

for (i in 1:nrow(train_data)){
  train_data$diagnosis[i] <-  ifelse(train_data$diagnosis[i] == "M", 1, 0)
}
train_data[,2] <-  as.integer(train_data[,2])
train_datasubset <-  train_data[,c(2:ncol(train_data))]

test_data = ovarian.dataset.test
for (i in 1:nrow(test_data)){
  test_data$diagnosis[i] <- ifelse(test_data$diagnosis[i] == "M", 1, 0)
}
test_data[,2] <-  as.integer(test_data[,2])
test_datasubset <-  test_data[, c(2:ncol(test_data))]

```

# 3.1
Building logistical classifier:

```{r}

glm.fit <-  glm(diagnosis ~ perimeter + area + smoothness + symmetry + concavity + protein1 + protein10 + protein11 + protein12 + protein13 + protein14 + protein15 + protein16 + protein17 + protein18 + protein19 + protein2 + protein20 + protein21 + protein22 + protein23 + protein24 + protein25 + protein3 + protein4 + protein5 + protein6 + protein7 + protein8 + protein9 , data = train_datasubset, family = binomial )
```

Fitting with the test data

```{r}
glm.probs <-  predict(glm.fit, test_datasubset, type = "response")

glm.pred <-  ifelse(glm.probs > 0.5, 1, 0)

conf_matrix <- table(glm.pred, test_data$diagnosis)

conf_matrix

TP <- conf_matrix[2, 2]
TN <- conf_matrix[1, 1]
FP <- conf_matrix[2, 1]
FN <- conf_matrix[1, 2]

accuracy <- (TP + TN) / sum(conf_matrix)

precision <- TP / (TP + FP)

recall <- TP / (TP + FN)

cat("Accuracy for test set: ", accuracy, "\n")
cat("Precision for test set: ", precision, "\n")
cat("Recall for test set: ", recall, "\n")

```

```{r}
glm.probs <-  predict(glm.fit, train_datasubset, type = "response")
glm.pred <-  ifelse(glm.probs > 0.5, 1, 0)

conf_matrix <- table(glm.pred, train_data$diagnosis)

conf_matrix

TP <- conf_matrix[2, 2]
TN <- conf_matrix[1, 1]
FP <- conf_matrix[2, 1]
FN <- conf_matrix[1, 2]

accuracy <- (TP + TN) / sum(conf_matrix)

precision <- TP / (TP + FP)

recall <- TP / (TP + FN)

cat("Accuracy for train set: ", accuracy, "\n")
cat("Precision for train set: ", precision, "\n")
cat("Recall for train set: ", recall, "\n")
```
The classifier performs better on the training set with a high accuracy (1) because the model was trained on this data, and it "memorizes" the training samples. On the other hand, the accuracy of the classifier on the test set is lower (0.98) because the test set contains data that the model has not seen during training, and the model may not perform as well on unseen data. Precision and recall on the training set are also high, which means that the model is good at classifying the training samples. 

# Q3.2

Applying PCA on the train data set

``` {r}
pca_train_result <-  prcomp(train_datasubset, center = TRUE, scale. = TRUE)

summary(pca_train_result)

top5PC_data_train <-  as.data.frame(pca_train_result$x[, 1:5])

top5PC_data_train$diagnosis <- train_datasubset$diagnosis

top5PC_data_train
```

Building logistical classifier and fit with the top 5 PCs (train data set):
```{r}
glmTop5.fit <-  glm(diagnosis ~ PC1 + PC2 + PC3 + PC4 + PC5, data = top5PC_data_train, family = binomial)

glmTop5.probs <-  predict(glmTop5.fit, top5PC_data_train, type = "response")

glmTop5.pred <-  ifelse(glmTop5.probs > 0.5, 1, 0)

conf_matrix_top5_pcs <- table(glmTop5.pred, top5PC_data_train$diagnosis)

conf_matrix_top5_pcs

TP_top5_pcs <- conf_matrix_top5_pcs[2, 2]
TN_top5_pcs <- conf_matrix_top5_pcs[1, 1]
FP_top5_pcs <- conf_matrix_top5_pcs[2, 1]
FN_top5_pcs <- conf_matrix_top5_pcs[1, 2]

accuracy_top5_pcs <- (TP_top5_pcs + TN_top5_pcs) / sum(conf_matrix_top5_pcs)

precision_top5_pcs <- TP_top5_pcs / (TP_top5_pcs + FP_top5_pcs)

recall_top5_pcs <- TP_top5_pcs / (TP_top5_pcs + FN_top5_pcs)

cat("Performance on Train Set with Top 5 PCs:\n")
cat("Accuracy: ", accuracy_top5_pcs, "\n")
cat("Precision: ", precision_top5_pcs, "\n")
cat("Recall: ", recall_top5_pcs, "\n")
```

Apply the same PCA to test data 
Building logistical classifier and fit with the top 5 PCs
```{r}
pca_test_result <- predict(pca_train_result, newdata = test_datasubset)

top5PC_data_test <- as.data.frame(pca_test_result[, 1:5])

top5PC_data_test$diagnosis <- test_datasubset$diagnosis

glmTop5_test_fit <- glm(diagnosis ~ PC1 + PC2 + PC3 + PC4 + PC5, data = top5PC_data_test, family = binomial)

glmTop5_test_probs <- predict(glmTop5_test_fit, top5PC_data_test, type = "response")

glmTop5_test_pred <- ifelse(glmTop5_test_probs > 0.5, 1, 0)

conf_matrix_top5_pcs <- table(glmTop5_test_pred, test_datasubset$diagnosis)

conf_matrix_top5_pcs

TP_top5_pcs <- conf_matrix_top5_pcs[2, 2]
TN_top5_pcs <- conf_matrix_top5_pcs[1, 1]
FP_top5_pcs <- conf_matrix_top5_pcs[2, 1]
FN_top5_pcs <- conf_matrix_top5_pcs[1, 2]

accuracy_top5_pcs <- (TP_top5_pcs + TN_top5_pcs) / sum(conf_matrix_top5_pcs)

precision_top5_pcs <- TP_top5_pcs / (TP_top5_pcs + FP_top5_pcs)

recall_top5_pcs <- TP_top5_pcs / (TP_top5_pcs + FN_top5_pcs)

cat("Performance on Test Set with Top 5 PCs:\n")
cat("Accuracy: ", accuracy_top5_pcs, "\n")
cat("Precision: ", precision_top5_pcs, "\n")
cat("Recall: ", recall_top5_pcs, "\n")
```
# Q3.3 

The result of applying the regression test on the PCA data has higher accuracy, precision, and recall than that of applying the regression test on the test data only. This may because PCA makes the difference between each characteristic more clear in each of their own dimension so that the regression model can fit the line in each of those dimensions more accurately.

# Q3.4 

The result of the general linear regression also has higher accuracy, precision, and recall in both applying the data directly and applying the methods to the PCs. 

# Q3.5 (working on this)

```{r}
library(ROCR)

pred.prob <- predict(glm.fit, ovarian.dataset, type="response")
predict <- prediction(pred.prob, ovarian.dataset$diagnosis, label.ordering=c("B","M"))
perform <- performance(predict,"tpr","fpr")
plot(perform,colorize=TRUE)
```
Looking at the ROC curve, there is little overlap between the two classes of TPR and FPR. The model's separability is high and performs well in separating the positive and negative classes. The ROC curve can give use a visualization of the trade of between the sensitivity/specifity of a model.

# Q3.6

Use decision tree as the extra classifier:

```{r}
# remember to install "rpart" packages
library(tidymodels)
library(rpart)

# scaling the datasets
train_datasubset_scaled = scale(train_datasubset)
test_datasubset_scaled = scale(test_datasubset)

# convert data into dataframes

train_datasubset_scaled_df = as.data.frame(train_datasubset_scaled)
test_datasubset_scaled_df = as.data.frame(test_datasubset_scaled)
# training the model


model_DT.fit = rpart(diagnosis ~ ., data = train_datasubset_scaled_df, method = 'anova')

# making predictions using the model
model_DT.probs = predict(model_DT.fit, newdata = test_datasubset_scaled_df, type = 'matrix')
model_DT.pred = ifelse(model_DT.probs > 0.5, 1, 0)

confusion_matrix_DT = table(model_DT.pred, test_datasubset_scaled_df$diagnosis)
confusion_matrix_DT

TP_DT <- confusion_matrix_DT[2, 2]
TN_DT <- confusion_matrix_DT[1, 1]
FP_DT <- confusion_matrix_DT[2, 1]
FN_DT <- confusion_matrix_DT[1, 2]

accuracy_DT <- (TP_DT + TN_DT) / sum(confusion_matrix_DT)

precision_DT <- TP_DT / (TP_DT + FP_DT)

recall_DT <- TP_DT / (TP_DT + FN_DT)

cat("Performance on Test Set directly onto the data PCs:\n")
cat("Accuracy: ", accuracy_DT, "\n")
cat("Precision: ", precision_DT, "\n")
cat("Recall: ", recall_DT, "\n")


```
Applying the decision tree classifier on the PC of the data

```{r}

model_DT_PC.fit = rpart(diagnosis ~ PC1 + PC2 + PC3 + PC4 + PC5, data = top5PC_data_train, method = 'anova')

model_DT_PC.probs = predict(model_DT_PC.fit, newdata = top5PC_data_test, type = 'matrix')
model_DT_PC.pred = ifelse(model_DT_PC.probs > 0.5, 1, 0)

confusion_matrix_DT_PC = table(model_DT_PC.pred, top5PC_data_test$diagnosis)
confusion_matrix_DT_PC

TP_DT_PC <- confusion_matrix_DT_PC[2, 2]
TN_DT_PC <- confusion_matrix_DT_PC[1, 1]
FP_DT_PC <- confusion_matrix_DT_PC[2, 1]
FN_DT_PC <- confusion_matrix_DT_PC[1, 2]

accuracy_DT_PC <- (TP_DT_PC + TN_DT_PC) / sum(confusion_matrix_DT_PC)

precision_DT_PC <- TP_DT_PC / (TP_DT_PC + FP_DT_PC)

recall_DT_PC <- TP_DT_PC / (TP_DT_PC + FN_DT_PC)

cat("Performance on Test Set with Top 5 PCs:\n")
cat("Accuracy: ", accuracy_DT_PC, "\n")
cat("Precision: ", precision_DT_PC, "\n")
cat("Recall: ", recall_DT_PC, "\n")

```
All of accuracy, precison, and recall improved compared to just applying the classifier directly onto the data.

Generating a ROC curve

```{r}
library(ROCR)

pred_DT.prob <- predict(model_DT.fit, ovarian.dataset, type='matrix')
predict_DT <- prediction(pred_DT.prob, ovarian.dataset$diagnosis, label.ordering=c("B","M"))
perform_DT <- performance(predict_DT,"tpr","fpr")
plot(perform_DT,colorize=TRUE)
```
THe model has no discrimination capacity and the rate betweet TPR and FPR are the same (resulting in a srtaight ROC curve)
